#!/usr/bin/env python3

import argparse
import socket
import threading
from urllib.parse import urlparse
from html.parser import HTMLParser

socket_timeout = 30
login_url = 'http://fring.ccs.neu.edu/accounts/login/?next=/fakebook/'
links_pending = ['http://fring.ccs.neu.edu/fakebook/']
links_completed = []
secret_flags = []

class MyHtmlParser(HTMLParser):
    def init(self):
        self.csrf_token = None

    def handle_starttag(self, tag, attrs):
        self.grab_csrf_token(tag, attrs)
        if tag == 'a' and attrs[0][1][0] == '/':
            new_link = 'http://fring.ccs.neu.edu' + attrs[0][1]
            if new_link in links_pending or new_link in links_completed:
                return
            links_pending.append(new_link)

    def handle_data(self, data):
        # print(data)
        if 'FLAG:' in data and data not in secret_flags:
            new_flag = data.split('FLAG: ')[1]
            secret_flags.append(new_flag)
            print(new_flag)

    def grab_csrf_token(self, html_tag, data):
        if html_tag != 'input' or ('name', "\\'csrfmiddlewaretoken\\'") not in data:
            return

        t1, t2, t3 = data

        if t1 != ('type', "\\'hidden\\'") and t2 != ('name', "\\'csrfmiddlewaretoken\\'"):
            return

        _, csrf_token = t3
        self.csrf_token = csrf_token.replace('\\\'', '')


def connect(host, port):
    my_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    my_socket.settimeout(socket_timeout)
    my_socket.connect((host, port))
    return my_socket

def receive(my_socket):
    try:
        chunks = []
        while(True):
            recv = my_socket.recv(10000)
            chunks.append(str(recv))
            if len(recv) == 0:
                my_socket.close()
                break
        return "".join(chunks)
    except:
      print("Connection timed out.")

def get_response_code(header):
    print(header[11:14])

def get_session_id(html):
    beginning = html.find('sessionid') + 10
    end = html.find('; expires=')
    session_id = html[beginning:end]
    return session_id

def login(username, password, csrf_token):
    form_data = "username={username}&password={password}&csrfmiddlewaretoken={csrftoken}&next=%2Ffakebook%2F\n\n".format(username=username, password=password, csrftoken=csrf_token)
    cookie = "csrftoken={token};".format(token=csrf_token)
    (status, response) = post_request("http://fring.ccs.neu.edu/accounts/login", "/accounts/login/", form_data, cookie)
    return response

def get_request(url, cookie):
    url_parts = urlparse(url)
    path, netloc = url_parts.path, url_parts.netloc
    my_socket = connect(netloc, 80)
    request = 'GET {} HTTP/1.1\r\nHost: {}\r\nCookie: {}\r\nKeep-Alive: timeout=10, max=10000000\r\nConnection: Close \r\n\r\n'.format(path, netloc, cookie)
    my_socket.sendall(request.encode())
    response_header = receive(my_socket)
    response_code = get_response_code(response_header)
    return (response_code, response_header)

def post_request(uri, path, form_data, cookie):
    host = urlparse(uri).netloc
    sock = connect(host, 80)
    header = """
POST {path} HTTP/1.1
Host: {host}
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
User-Agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Mobile Safari/537.36
Cookie: {cookie}
Accept-Language: en-US,en;q=0.5
Accept-Encoding: identity
Cache-Control: no-cache
Origin: http://fring.ccs.neu.edu
Pragma: no-cache
Referer: http://fring.ccs.neu.edu/accounts/login/?next=/fakebook/
Upgrade-Insecure-Requests: 1
Connection: Close
Content-Type: application/x-www-form-urlencoded
""".format(path=path, host=host, cookie=cookie)
    content_length = "Content-Length: " + str(len(form_data)) + "\n\n"
    request = header + content_length + form_data
    sock.sendall(request.encode())
    response = receive(sock)
    status = get_response_code(response)
    sock.close()
    return (status, response)

def crawl(my_parser, session_cookie):
    global links_pending
    global links_completed
    if len(links_pending) > 0:
        link = links_pending.pop()
        if link is None or link in links_completed or link in links_pending:
            return
        (status, response) = get_request(link, session_cookie)
        if status == 301:
            print(response)
        elif status in [403, 404]:
            print('403/404')
            return
        elif status == 500:
            print('500')
            links_pending.append(link)
            return
        else:
            try:
                my_parser.feed(response)
            except:
                pass
            if link not in links_completed:
                links_completed.append(link)

def start_crawling(my_parser, session_cookie):
    threads = []
    while len(secret_flags) < 5:
        if len(links_pending) == 0 or threading.active_count() > 20:
            continue
        try:
            thread = threading.Thread(target=crawl, args=(my_parser, session_cookie))
            thread.start()
            threads.append(thread)
        except:
            pass


def main(username, password):
    inital_status, intial_load = get_request(login_url, None)
    my_parser = MyHtmlParser()
    my_parser.feed(intial_load)
    csrf_token = my_parser.csrf_token
    login_reponse = login(username, password, csrf_token)
    session_id = get_session_id(login_reponse)
    session_cookie = 'csrftoken={};  sessionid={}'.format(csrf_token, session_id)
    start_crawling(my_parser, session_cookie)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='web crawler')
    parser.add_argument('username', metavar='username', type=str, help="your username")
    parser.add_argument('password', metavar='password', type=str, help="your password")
    args = parser.parse_args()
    main(args.username, args.password)
